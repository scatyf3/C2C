2025-12-19 20:31:40,024 - datasets - DEBUG - PyTorch version 2.6.0 available.
2025-12-19 20:31:40,814 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-12-19 20:31:40,891 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/nics-efc/C2C_Fuser/revision/main HTTP/1.1" 200 30238
Fetching 59 files:   0%|          | 0/59 [00:00<?, ?it/s]Fetching 59 files: 100%|██████████| 59/59 [00:00<00:00, 6178.10it/s]
2025-12-19 20:31:40,911 - script.playground.inference_example - INFO - Loading Rosetta model with SLM: Qwen/Qwen3-0.6B, LLM: Qwen/Qwen2.5-0.5B-Instruct, checkpoints from: /scratch/yf3005/.cache/huggingface/hub/models--nics-efc--C2C_Fuser/snapshots/909c76d71cfb946171aa2ab7912dd07c0ba2e995/qwen3_0.6b+qwen2.5_0.5b_Fuser/final
2025-12-19 20:31:40,935 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /Qwen/Qwen3-0.6B/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-12-19 20:31:40,941 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/Qwen/Qwen3-0.6B/c1899de289a04d12100db370d81485cdf75e47ca/tokenizer_config.json HTTP/1.1" 200 0
2025-12-19 20:31:40,974 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/Qwen/Qwen3-0.6B/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-12-19 20:31:41,424 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /Qwen/Qwen3-0.6B/resolve/main/config.json HTTP/1.1" 307 0
2025-12-19 20:31:41,430 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/Qwen/Qwen3-0.6B/c1899de289a04d12100db370d81485cdf75e47ca/config.json HTTP/1.1" 200 0
2025-12-19 20:31:42,762 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /Qwen/Qwen3-0.6B/resolve/main/generation_config.json HTTP/1.1" 307 0
2025-12-19 20:31:42,767 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/Qwen/Qwen3-0.6B/c1899de289a04d12100db370d81485cdf75e47ca/generation_config.json HTTP/1.1" 200 0
2025-12-19 20:31:42,794 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /Qwen/Qwen3-0.6B/resolve/main/custom_generate/generate.py HTTP/1.1" 404 0
2025-12-19 20:31:42,825 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /Qwen/Qwen2.5-0.5B-Instruct/resolve/main/config.json HTTP/1.1" 307 0
2025-12-19 20:31:42,830 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/Qwen/Qwen2.5-0.5B-Instruct/7ae557604adf67be50417f59c2c2f167def9a775/config.json HTTP/1.1" 200 0
2025-12-19 20:31:43,645 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /Qwen/Qwen2.5-0.5B-Instruct/resolve/main/generation_config.json HTTP/1.1" 307 0
2025-12-19 20:31:43,650 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/Qwen/Qwen2.5-0.5B-Instruct/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json HTTP/1.1" 200 0
2025-12-19 20:31:43,674 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /Qwen/Qwen2.5-0.5B-Instruct/resolve/main/custom_generate/generate.py HTTP/1.1" 404 0
2025-12-19 20:31:51,209 - script.playground.inference_example - INFO - Loading projectors...[C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
), C2CProjector(
  (key_in): Linear(in_features=1152, out_features=1024, bias=True)
  (value_in): Linear(in_features=1152, out_features=1024, bias=True)
  (key_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_mlp1): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_scalar_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (value_scalar_head): Linear(in_features=1024, out_features=8, bias=True)
  (key_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (value_proj_mlp2): RegularMLP(
    (blocks): ModuleList(
      (0): StandardFFNLayer(
        (norm): RMSNorm((1024,), eps=1e-06, elementwise_affine=True)
        (w1): Linear(in_features=1024, out_features=1024, bias=False)
        (w2): Linear(in_features=1024, out_features=1024, bias=False)
        (drop): Dropout(p=0.1, inplace=False)
        (act): GELU(approximate='none')
      )
    )
  )
  (key_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
  (value_proj_out): Linear(in_features=1024, out_features=1024, bias=True)
)]
2025-12-19 20:31:51,211 - script.playground.inference_example - INFO - Loading aggregators...[]
2025-12-19 20:31:51,248 - script.playground.inference_example - INFO - Rosetta model loaded successfully:RosettaModel(
  models=2,
    [0] Qwen3ForCausalLM (base)
    [1] Qwen2ForCausalLM
  projectors=28 (mappings=0),
  aggregators=0,
  fusion_mode='parallel',
  kv_cache: empty,
  device=cuda:0
)
2025-12-19 20:31:51,271 - rosetta.model.wrapper - DEBUG - Starting generation with max_new_tokens=256, do_sample=False, temperature=1.0
2025-12-19 20:31:51,271 - rosetta.model.wrapper - DEBUG - Starting prefill phase with prompt length=19
2025-12-19 20:31:51,274 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0],
         [1, 0]]], device='cuda:0'), tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:51,274 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=19, num_sections=2
2025-12-19 20:31:51,274 - rosetta.model.wrapper - DEBUG - parsing section lengths: [18, 1]
2025-12-19 20:31:51,274 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 18, 19]
2025-12-19 20:31:51,274 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:51,274 - rosetta.model.wrapper - DEBUG - Processing section 1/2
2025-12-19 20:31:51,274 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 18]), attention_mask shape=torch.Size([1, 18]), position_ids shape=None, labels shape=None
2025-12-19 20:31:51,967 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:51,967 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 18, 128]), value_shape: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:51,967 - rosetta.model.wrapper - DEBUG - if this is not the last section, process source models for kv-cache projection
2025-12-19 20:31:51,967 - rosetta.model.wrapper - DEBUG - Processing source model's attention mask for kv-cache projection
2025-12-19 20:31:52,047 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=1
2025-12-19 20:31:52,047 - rosetta.model.wrapper - DEBUG - >0: Bitmask selecting sharers (1 (001)=sharer1, 2 (010)=sharer2, 3 (011)=both, 7 (111)=all three)
2025-12-19 20:31:52,048 - rosetta.model.wrapper - DEBUG - iterating over source models for projection
2025-12-19 20:31:52,048 - rosetta.model.wrapper - DEBUG - Processing source model in each target layer  for projection
2025-12-19 20:31:52,048 - rosetta.model.wrapper - DEBUG - Target layer 0 with projector entry: [[0, 0]]
2025-12-19 20:31:52,048 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,048 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,048 - rosetta.model.wrapper - DEBUG - Using source model layer 0 with projector 0
2025-12-19 20:31:52,068 - rosetta.model.wrapper - DEBUG - Target layer 1 with projector entry: [[0, 1]]
2025-12-19 20:31:52,068 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,068 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,068 - rosetta.model.wrapper - DEBUG - Using source model layer 0 with projector 1
2025-12-19 20:31:52,072 - rosetta.model.wrapper - DEBUG - Target layer 2 with projector entry: [[0, 2]]
2025-12-19 20:31:52,072 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,072 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,072 - rosetta.model.wrapper - DEBUG - Using source model layer 0 with projector 2
2025-12-19 20:31:52,076 - rosetta.model.wrapper - DEBUG - Target layer 3 with projector entry: [[0, 3]]
2025-12-19 20:31:52,076 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,076 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,076 - rosetta.model.wrapper - DEBUG - Using source model layer 0 with projector 3
2025-12-19 20:31:52,080 - rosetta.model.wrapper - DEBUG - Target layer 4 with projector entry: [[0, 4]]
2025-12-19 20:31:52,080 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,080 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,080 - rosetta.model.wrapper - DEBUG - Using source model layer 0 with projector 4
2025-12-19 20:31:52,084 - rosetta.model.wrapper - DEBUG - Target layer 5 with projector entry: [[1, 5]]
2025-12-19 20:31:52,084 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,084 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,084 - rosetta.model.wrapper - DEBUG - Using source model layer 1 with projector 5
2025-12-19 20:31:52,088 - rosetta.model.wrapper - DEBUG - Target layer 6 with projector entry: [[2, 6]]
2025-12-19 20:31:52,088 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,088 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,088 - rosetta.model.wrapper - DEBUG - Using source model layer 2 with projector 6
2025-12-19 20:31:52,092 - rosetta.model.wrapper - DEBUG - Target layer 7 with projector entry: [[3, 7]]
2025-12-19 20:31:52,092 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,092 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,092 - rosetta.model.wrapper - DEBUG - Using source model layer 3 with projector 7
2025-12-19 20:31:52,096 - rosetta.model.wrapper - DEBUG - Target layer 8 with projector entry: [[4, 8]]
2025-12-19 20:31:52,096 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,096 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,096 - rosetta.model.wrapper - DEBUG - Using source model layer 4 with projector 8
2025-12-19 20:31:52,100 - rosetta.model.wrapper - DEBUG - Target layer 9 with projector entry: [[5, 9]]
2025-12-19 20:31:52,100 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,100 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,100 - rosetta.model.wrapper - DEBUG - Using source model layer 5 with projector 9
2025-12-19 20:31:52,104 - rosetta.model.wrapper - DEBUG - Target layer 10 with projector entry: [[6, 10]]
2025-12-19 20:31:52,104 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,104 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,104 - rosetta.model.wrapper - DEBUG - Using source model layer 6 with projector 10
2025-12-19 20:31:52,108 - rosetta.model.wrapper - DEBUG - Target layer 11 with projector entry: [[7, 11]]
2025-12-19 20:31:52,108 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,108 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,108 - rosetta.model.wrapper - DEBUG - Using source model layer 7 with projector 11
2025-12-19 20:31:52,112 - rosetta.model.wrapper - DEBUG - Target layer 12 with projector entry: [[8, 12]]
2025-12-19 20:31:52,112 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,112 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,112 - rosetta.model.wrapper - DEBUG - Using source model layer 8 with projector 12
2025-12-19 20:31:52,115 - rosetta.model.wrapper - DEBUG - Target layer 13 with projector entry: [[9, 13]]
2025-12-19 20:31:52,116 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,116 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,116 - rosetta.model.wrapper - DEBUG - Using source model layer 9 with projector 13
2025-12-19 20:31:52,119 - rosetta.model.wrapper - DEBUG - Target layer 14 with projector entry: [[10, 14]]
2025-12-19 20:31:52,119 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,120 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,120 - rosetta.model.wrapper - DEBUG - Using source model layer 10 with projector 14
2025-12-19 20:31:52,123 - rosetta.model.wrapper - DEBUG - Target layer 15 with projector entry: [[11, 15]]
2025-12-19 20:31:52,123 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,123 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,123 - rosetta.model.wrapper - DEBUG - Using source model layer 11 with projector 15
2025-12-19 20:31:52,127 - rosetta.model.wrapper - DEBUG - Target layer 16 with projector entry: [[12, 16]]
2025-12-19 20:31:52,128 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,128 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,128 - rosetta.model.wrapper - DEBUG - Using source model layer 12 with projector 16
2025-12-19 20:31:52,131 - rosetta.model.wrapper - DEBUG - Target layer 17 with projector entry: [[13, 17]]
2025-12-19 20:31:52,131 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,132 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,132 - rosetta.model.wrapper - DEBUG - Using source model layer 13 with projector 17
2025-12-19 20:31:52,135 - rosetta.model.wrapper - DEBUG - Target layer 18 with projector entry: [[14, 18]]
2025-12-19 20:31:52,135 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,135 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,135 - rosetta.model.wrapper - DEBUG - Using source model layer 14 with projector 18
2025-12-19 20:31:52,139 - rosetta.model.wrapper - DEBUG - Target layer 19 with projector entry: [[15, 19]]
2025-12-19 20:31:52,139 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,139 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,139 - rosetta.model.wrapper - DEBUG - Using source model layer 15 with projector 19
2025-12-19 20:31:52,143 - rosetta.model.wrapper - DEBUG - Target layer 20 with projector entry: [[16, 20]]
2025-12-19 20:31:52,143 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,143 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,143 - rosetta.model.wrapper - DEBUG - Using source model layer 16 with projector 20
2025-12-19 20:31:52,147 - rosetta.model.wrapper - DEBUG - Target layer 21 with projector entry: [[17, 21]]
2025-12-19 20:31:52,147 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,147 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,147 - rosetta.model.wrapper - DEBUG - Using source model layer 17 with projector 21
2025-12-19 20:31:52,151 - rosetta.model.wrapper - DEBUG - Target layer 22 with projector entry: [[18, 22]]
2025-12-19 20:31:52,151 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,151 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,151 - rosetta.model.wrapper - DEBUG - Using source model layer 18 with projector 22
2025-12-19 20:31:52,155 - rosetta.model.wrapper - DEBUG - Target layer 23 with projector entry: [[19, 23]]
2025-12-19 20:31:52,155 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,155 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,155 - rosetta.model.wrapper - DEBUG - Using source model layer 19 with projector 23
2025-12-19 20:31:52,159 - rosetta.model.wrapper - DEBUG - Target layer 24 with projector entry: [[20, 24]]
2025-12-19 20:31:52,159 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,159 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,159 - rosetta.model.wrapper - DEBUG - Using source model layer 20 with projector 24
2025-12-19 20:31:52,163 - rosetta.model.wrapper - DEBUG - Target layer 25 with projector entry: [[21, 25]]
2025-12-19 20:31:52,163 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,163 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,163 - rosetta.model.wrapper - DEBUG - Using source model layer 21 with projector 25
2025-12-19 20:31:52,167 - rosetta.model.wrapper - DEBUG - Target layer 26 with projector entry: [[22, 26]]
2025-12-19 20:31:52,167 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,167 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,167 - rosetta.model.wrapper - DEBUG - Using source model layer 22 with projector 26
2025-12-19 20:31:52,171 - rosetta.model.wrapper - DEBUG - Target layer 27 with projector entry: [[23, 27]]
2025-12-19 20:31:52,171 - rosetta.model.wrapper - DEBUG - New base KV cache slice shapes - key: torch.Size([1, 8, 18, 128]), value: torch.Size([1, 8, 18, 128])
2025-12-19 20:31:52,171 - rosetta.model.wrapper - DEBUG - Start KV Cache Projection
2025-12-19 20:31:52,171 - rosetta.model.wrapper - DEBUG - Using source model layer 23 with projector 27
2025-12-19 20:31:52,176 - rosetta.model.wrapper - DEBUG - Processing section 2/2
2025-12-19 20:31:52,176 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 19]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,237 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 2
2025-12-19 20:31:52,237 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 19, 128]), value_shape: torch.Size([1, 8, 19, 128])
2025-12-19 20:31:52,237 - rosetta.model.wrapper - DEBUG - Section 2: sharer_mask=-1
2025-12-19 20:31:52,237 - rosetta.model.wrapper - DEBUG - Prefill phase completed
2025-12-19 20:31:52,254 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,254 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,255 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,255 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,255 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,255 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,255 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,290 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,290 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 20, 128]), value_shape: torch.Size([1, 8, 20, 128])
2025-12-19 20:31:52,290 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,291 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,291 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,291 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,291 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,291 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,291 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,291 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,326 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,326 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 21, 128]), value_shape: torch.Size([1, 8, 21, 128])
2025-12-19 20:31:52,326 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,327 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,327 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,327 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,327 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,327 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,327 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,327 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,362 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,362 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 22, 128]), value_shape: torch.Size([1, 8, 22, 128])
2025-12-19 20:31:52,362 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,363 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,363 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,363 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,363 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,363 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,363 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,363 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,398 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,398 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 23, 128]), value_shape: torch.Size([1, 8, 23, 128])
2025-12-19 20:31:52,398 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,399 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,399 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,399 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,399 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,399 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,399 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,399 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,434 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,434 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 24, 128]), value_shape: torch.Size([1, 8, 24, 128])
2025-12-19 20:31:52,434 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,435 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,435 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,435 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,435 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,435 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,435 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,435 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,470 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,470 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 25, 128]), value_shape: torch.Size([1, 8, 25, 128])
2025-12-19 20:31:52,470 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,471 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,471 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,471 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,471 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,471 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,471 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,471 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,506 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,506 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 26, 128]), value_shape: torch.Size([1, 8, 26, 128])
2025-12-19 20:31:52,506 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,507 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,507 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,507 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,507 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,507 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,507 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,507 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,542 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,542 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 27, 128]), value_shape: torch.Size([1, 8, 27, 128])
2025-12-19 20:31:52,542 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,543 - rosetta.model.wrapper - DEBUG - RosettaModel forward called, with kv_cache_index=[tensor([[[-1,  0]]], device='cuda:0')]
2025-12-19 20:31:52,543 - rosetta.model.wrapper - DEBUG - RosettaModel forward: seqlen=1, num_sections=1
2025-12-19 20:31:52,543 - rosetta.model.wrapper - DEBUG - parsing section lengths: [1]
2025-12-19 20:31:52,543 - rosetta.model.wrapper - DEBUG - computed section starts: [0, 1]
2025-12-19 20:31:52,543 - rosetta.model.wrapper - DEBUG - Entering prefill phase, fuse kv cache accourding to kv_cache_index
2025-12-19 20:31:52,543 - rosetta.model.wrapper - DEBUG - Processing section 1/1
2025-12-19 20:31:52,543 - rosetta.model.wrapper - DEBUG - Prefill inputs: input_ids shape=torch.Size([1, 1]), attention_mask shape=torch.Size([1, 1]), position_ids shape=None, labels shape=None
2025-12-19 20:31:52,578 - rosetta.model.wrapper - DEBUG - Base model output obtained for section 1
2025-12-19 20:31:52,578 - rosetta.model.wrapper - DEBUG - Current KV Cache for base model - num_layers: 28, key_shape: torch.Size([1, 8, 28, 128]), value_shape: torch.Size([1, 8, 28, 128])
2025-12-19 20:31:52,578 - rosetta.model.wrapper - DEBUG - Section 1: sharer_mask=-1
2025-12-19 20:31:52,578 - rosetta.model.wrapper - DEBUG - Generation completed: generated 10 tokens
Model Qwen/Qwen3-0.6B already has a chat template.
C2C output text: Hello! How can I assist you today?
